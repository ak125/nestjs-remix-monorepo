# ðŸ§ª A/B Testing du Crawl Budget

## ðŸ“‹ Concept

SystÃ¨me d'expÃ©rimentation pour mesurer l'impact d'**inclure/exclure/rÃ©duire** certaines familles de produits dans le sitemap sur:
- ðŸ“Š **Taux d'indexation** (via Google Search Console)
- ðŸ¤– **Crawl rate** (pages crawlÃ©es/jour)
- ðŸ“ˆ **Trafic organique** (via Google Analytics)
- ðŸŽ¯ **Positionnement** (ranking moyen)

## ðŸŽ¯ Cas d'usage

### 1. **Exclure familles Ã  faible valeur**
```json
{
  "name": "Test exclusion pneus anciens",
  "action": "exclude",
  "targetFamilies": ["PNEU_VIEUX", "PNEU_OCCASION"],
  "durationDays": 30
}
```
**HypothÃ¨se**: Enlever 10 000 URLs de pneus anciens permet Ã  Google de crawler plus de pages Ã  forte valeur.

### 2. **Inclure nouvelles familles**
```json
{
  "name": "Test inclusion accessoires connectÃ©s",
  "action": "include",
  "targetFamilies": ["ACCESS_CONNECTE", "DIAG_BLUETOOTH"],
  "durationDays": 45
}
```
**HypothÃ¨se**: Ajouter 2 000 URLs d'accessoires connectÃ©s augmente le trafic organique.

### 3. **RÃ©duire familles volumineuses**
```json
{
  "name": "Test rÃ©duction gamme moteur 50%",
  "action": "reduce",
  "targetFamilies": ["PIECE_MOTEUR"],
  "reductionPercent": 50,
  "durationDays": 60
}
```
**HypothÃ¨se**: Ne garder que les 50% de piÃ¨ces moteur les plus demandÃ©es amÃ©liore la qualitÃ© du sitemap.

## ðŸš€ Workflow complet

### Phase 1: CrÃ©ation (DRAFT)
```bash
POST /seo-logs/crawl-budget/experiments
{
  "name": "Exclusion pneus anciens",
  "action": "exclude",
  "targetFamilies": ["PNEU_VIEUX"],
  "durationDays": 30
}
```
â†’ **Collecte baseline automatique** (30j avant)
â†’ GÃ©nÃ¨re sitemap filtrÃ©

### Phase 2: Activation (RUNNING)
```bash
# 1. TÃ©lÃ©charger sitemap filtrÃ©
GET /seo-logs/crawl-budget/experiments/{id}/sitemap
# Sauvegarder en sitemap-experiment.xml

# 2. Soumettre Ã  Google Search Console
# https://search.google.com/search-console/sitemaps

# 3. Activer l'expÃ©rience
PATCH /seo-logs/crawl-budget/experiments/{id}/status
{ "status": "running" }
```

### Phase 3: Monitoring
```bash
# MÃ©triques quotidiennes
GET /seo-logs/crawl-budget/experiments/{id}/metrics?period=7d

# Comparaison temps rÃ©el
GET /seo-logs/crawl-budget/experiments/{id}/comparison
```

### Phase 4: Analyse
```bash
# Recommandations automatiques
GET /seo-logs/crawl-budget/experiments/{id}/recommendations

# RÃ©ponse:
{
  "recommendations": [
    {
      "action": "KEEP_EXCLUSION",
      "reason": "L'indexation s'est amÃ©liorÃ©e de +12%",
      "confidence": 0.95
    }
  ]
}
```

### Phase 5: ComplÃ©tion
```bash
PATCH /seo-logs/crawl-budget/experiments/{id}/status
{ "status": "completed" }
```

## ðŸ“Š MÃ©triques collectÃ©es

### 1. Crawl Stats (Google Search Console API)
- `totalCrawledUrls`: Pages crawlÃ©es totales
- `crawlRequestsCount`: RequÃªtes Googlebot/jour
- `avgCrawlRate`: Taux de crawl moyen

### 2. Indexation (site: operator ou GSC API)
- `indexedUrls`: Pages indexÃ©es
- `indexationRate`: % d'URLs indexÃ©es
- `indexationTime`: DÃ©lai moyen d'indexation

### 3. Trafic (Google Analytics)
- `organicSessions`: Sessions organiques
- `organicConversions`: Conversions SEO
- `avgPosition`: Position moyenne (GSC)

### 4. Par famille
```json
{
  "familyMetrics": [
    {
      "familyCode": "PNEU_VIEUX",
      "crawledUrls": 450,
      "indexedUrls": 380,
      "avgPosition": 45.2
    }
  ]
}
```

## ðŸŽ›ï¸ Statuts d'expÃ©rience

| Statut | Description | Actions disponibles |
|--------|-------------|---------------------|
| `DRAFT` | CrÃ©Ã©e mais non dÃ©marrÃ©e | Modifier, Activer |
| `RUNNING` | En cours | Pause, ComplÃ©ter |
| `PAUSED` | Mise en pause | Reprendre, ComplÃ©ter |
| `COMPLETED` | TerminÃ©e | Voir rapport final |

## ðŸ§  Logique de recommandations

```typescript
// AmÃ©lioration indexation > 5%
if (indexationRateChange > 5) {
  return "KEEP_EXCLUSION"; // Garder changement
}

// Chute trafic > 10%
if (organicSessionsChange < -10) {
  return "REVERT"; // RÃ©intÃ©grer familles
}

// Pas d'impact significatif
if (abs(indexationRateChange) < 2 && abs(organicSessionsChange) < 5) {
  return "NEUTRAL"; // DÃ©cision manuelle
}
```

## ðŸ“ˆ Exemple de rÃ©sultats

### ExpÃ©rience: Exclusion pneus anciens (10 000 URLs)

**Baseline (30j avant)**:
- Crawl rate: 1 200 pages/jour
- Indexation: 85 000 pages (82%)
- Trafic organique: 4 500 sessions/jour

**Pendant expÃ©rience (30j)**:
- Crawl rate: 1 450 pages/jour (**+21%**)
- Indexation: 86 200 pages (**+1.4%**)
- Trafic organique: 4 480 sessions/jour (**-0.4%**)

**Recommandation**: âœ… **KEEP_EXCLUSION** (confidence 90%)
- Le crawl budget s'est amÃ©liorÃ© significativement
- L'indexation a lÃ©gÃ¨rement augmentÃ©
- Le trafic est restÃ© stable (familles Ã  faible trafic)

## ðŸ”— IntÃ©gration APIs

### Google Search Console API
```typescript
// RÃ©cupÃ©rer stats de crawl
GET https://searchconsole.googleapis.com/v1/urlTestingTools/mobileFriendlyTest:run
GET https://www.googleapis.com/webmasters/v3/sites/{siteUrl}/sitemaps

// Indexation stats
GET https://searchconsole.googleapis.com/v1/searchanalytics/query
```

### Google Analytics 4 API
```typescript
// Trafic organique
POST https://analyticsdata.googleapis.com/v1beta/properties/{propertyId}:runReport
{
  "dimensions": [{"name": "sessionSource"}],
  "metrics": [{"name": "sessions"}],
  "dimensionFilter": {
    "filter": {"fieldName": "sessionSource", "stringFilter": {"value": "google"}}
  }
}
```

## ðŸ› ï¸ ImplÃ©mentation technique

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CrawlBudgetExperimentController        â”‚
â”‚  - POST /experiments                    â”‚
â”‚  - GET  /experiments/:id/metrics        â”‚
â”‚  - GET  /experiments/:id/comparison     â”‚
â”‚  - GET  /experiments/:id/sitemap        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CrawlBudgetExperimentService           â”‚
â”‚  - createExperiment()                   â”‚
â”‚  - generateFilteredSitemap()            â”‚
â”‚  - collectBaselineMetrics()             â”‚
â”‚  - getRecommendations()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prisma  â”‚     â”‚   APIs   â”‚
â”‚   DB    â”‚     â”‚  GSC+GA4 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Table Prisma (Ã  crÃ©er)
```prisma
model CrawlBudgetExperiment {
  id              String   @id @default(uuid())
  name            String
  description     String?
  action          String   // exclude, include, reduce
  targetFamilies  String[] // Array de codes gammes
  reductionPercent Int?
  durationDays    Int      @default(30)
  status          String   @default("draft")
  baseline        Json?    // MÃ©triques 30j avant
  startedAt       DateTime?
  completedAt     DateTime?
  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt

  metrics CrawlBudgetMetric[]

  @@map("crawl_budget_experiments")
}

model CrawlBudgetMetric {
  id                  String   @id @default(uuid())
  experimentId        String
  date                DateTime
  totalCrawledUrls    Int
  crawlRequestsCount  Int
  avgCrawlRate        Float
  indexedUrls         Int
  indexationRate      Float
  organicSessions     Int?
  organicConversions  Int?
  familyMetrics       Json?    // DÃ©tails par famille

  experiment CrawlBudgetExperiment @relation(fields: [experimentId], references: [id])

  @@unique([experimentId, date])
  @@map("crawl_budget_metrics")
}
```

## ðŸ“… Roadmap

- [ ] **Phase 1**: Structure de base (controller + service)
- [ ] **Phase 2**: Prisma schema + migrations
- [ ] **Phase 3**: IntÃ©gration Google Search Console API
- [ ] **Phase 4**: Collecte baseline automatique
- [ ] **Phase 5**: GÃ©nÃ©ration sitemap filtrÃ© dynamique
- [ ] **Phase 6**: Google Analytics 4 API
- [ ] **Phase 7**: Recommandations ML (TensorFlow.js)
- [ ] **Phase 8**: Dashboard Grafana

## ðŸŽ“ Best practices

1. **DurÃ©e minimale**: 30 jours (laisser Google re-crawler)
2. **Une expÃ©rience Ã  la fois**: Ã‰viter variables confondantes
3. **Baseline solide**: Attendre stabilitÃ© avant de lancer
4. **Seuil significatif**: Ignorer variations < 5%
5. **Combiner mÃ©triques**: Ne pas se fier qu'Ã  une seule mÃ©trique

## ðŸ“š RÃ©fÃ©rences

- [Google Search Console API](https://developers.google.com/webmaster-tools)
- [Google Analytics 4 API](https://developers.google.com/analytics/devguides/reporting/data/v1)
- [Sitemap Protocol](https://www.sitemaps.org/protocol.html)
