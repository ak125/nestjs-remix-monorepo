# üöÄ Guide de D√©marrage Ultra-Rapide - G√©n√©rateur IA GRATUIT

## üéâ AUCUNE API PAYANTE REQUISE !

Vous avez **3 options 100% GRATUITES** :

| Option | Co√ªt | Vitesse | Installation | Qualit√© |
|--------|------|---------|--------------|---------|
| **Ollama** üåü | **Gratuit ‚àû** | ‚ö°‚ö° Rapide | 5 min | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Groq** ‚ö° | **Gratuit** | ‚ö°‚ö°‚ö° Ultra | 2 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **HuggingFace** ü§ó | **Gratuit** | ‚ö° Moyen | 3 min | ‚≠ê‚≠ê‚≠ê |

---

## ‚ö° Installation Express (5 minutes)

### Option A : Script Automatique (RECOMMAND√â)

```bash
# 1. Ex√©cuter le script d'installation
./install-ai-providers.sh

# 2. Attendre 2-3 minutes (t√©l√©chargement du mod√®le)

# 3. C'est pr√™t ! üéâ
```

### Option B : Manuel (Ollama)

```bash
# 1. Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. D√©marrer le service
ollama serve &

# 3. T√©l√©charger un mod√®le
ollama pull llama3.1:8b    # Recommand√© (4.7GB)
# OU
ollama pull llama3.2:3b    # Plus l√©ger (2GB)

# 4. Configurer .env
cat >> .env << EOF
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
EOF
```

### Option C : Groq API (Ultra-Rapide)

```bash
# 1. Cr√©er un compte gratuit (2 min)
# üëâ https://console.groq.com

# 2. G√©n√©rer une cl√© API gratuite

# 3. Configurer .env
cat >> .env << EOF
AI_PROVIDER=groq
GROQ_API_KEY=gsk_votre_cl√©_ici
GROQ_MODEL=llama3-70b-8192
EOF
```

---

## üéØ Configuration Optimale (Auto-D√©tection)

**Utilisez tous les providers disponibles en fallback :**

```bash
# Dans votre .env
AI_PROVIDER=auto

# Ollama (local - priorit√© 1)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Groq (gratuit - priorit√© 2)
GROQ_API_KEY=gsk_votre_cl√©_ici
GROQ_MODEL=llama3-70b-8192

# HuggingFace (gratuit - priorit√© 3)
HUGGINGFACE_API_KEY=hf_votre_cl√©_ici
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

Le syst√®me essaiera automatiquement dans cet ordre.

---

## ‚úÖ Test de Fonctionnement

### 1. Test Ollama

```bash
# V√©rifier que le service fonctionne
curl http://localhost:11434/api/tags

# Test de g√©n√©ration direct
ollama run llama3.1:8b "√âcris une description professionnelle pour une vanne papillon motoris√©e DN50"
```

### 2. Test de l'API Backend

```bash
# G√©n√©rer une description de produit
curl -X POST http://localhost:5001/api/ai-content/generate/product-description \
  -H "Content-Type: application/json" \
  -d '{
    "productName": "Vanne papillon motoris√©e DN50",
    "category": "Vannes",
    "features": [
      "Corps en fonte GGG40",
      "Disque inox 316L",
      "Motorisation 24V DC"
    ],
    "tone": "professional",
    "length": "medium"
  }'
```

### 3. Test M√©ta SEO

```bash
curl -X POST http://localhost:5001/api/ai-content/generate/seo-meta \
  -H "Content-Type: application/json" \
  -d '{
    "pageTitle": "Vannes papillon motoris√©es - Catalogue 2025",
    "targetKeyword": "vanne papillon motoris√©e",
    "keywords": ["automatisation", "robinet industriel"]
  }'
```

---

## üé® Utilisation Frontend

### Composant G√©n√©rique

```typescript
import { AiContentGenerator } from '~/components/ai/AiContentGenerator';

export default function AdminContentPage() {
  return (
    <div className="container mx-auto py-8">
      <h1 className="text-3xl font-bold mb-6">
        G√©n√©rateur de Contenu IA
      </h1>
      
      <AiContentGenerator
        onContentGenerated={(content) => {
          console.log('Contenu:', content);
          // Utiliser le contenu
        }}
      />
    </div>
  );
}
```

### Composant Produit

```typescript
import { ProductDescriptionGenerator } from '~/components/ai/ProductDescriptionGenerator';

export default function ProductEditPage() {
  return (
    <ProductDescriptionGenerator
      productName="Vanne papillon DN50"
      onGenerated={(description) => {
        // Sauvegarder la description
        updateProduct({ description });
      }}
    />
  );
}
```

### Hook React

```typescript
import { useAiContent } from '~/hooks/useAiContent';

function MyComponent() {
  const { 
    generateProductDescription, 
    isLoading, 
    error 
  } = useAiContent();

  const handleGenerate = async () => {
    const result = await generateProductDescription({
      productName: 'Vanne papillon DN50',
      features: ['Corps fonte', 'Motorisation 24V'],
      tone: 'professional',
      length: 'medium',
    });
    
    console.log(result.content);
  };

  return (
    <button onClick={handleGenerate} disabled={isLoading}>
      {isLoading ? 'G√©n√©ration...' : '‚ú® G√©n√©rer avec IA'}
    </button>
  );
}
```

---

## üî• Exemples d'Utilisation Avanc√©e

### G√©n√©ration en Masse

```typescript
// G√©n√©rer des descriptions pour tous les produits
async function generateAllDescriptions(products: Product[]) {
  const { generateProductDescription } = useAiContent();
  
  for (const product of products) {
    const result = await generateProductDescription({
      productName: product.name,
      category: product.category,
      features: product.features,
      tone: 'professional',
      length: 'medium',
    });
    
    await updateProduct(product.id, {
      description: result.content
    });
  }
}
```

### G√©n√©ration par Lots

```bash
curl -X POST http://localhost:5001/api/ai-content/generate/batch \
  -H "Content-Type: application/json" \
  -d '{
    "requests": [
      {
        "type": "product_description",
        "prompt": "Vanne papillon DN50",
        "tone": "professional"
      },
      {
        "type": "seo_meta",
        "prompt": "Page catalogue vannes",
        "tone": "professional"
      }
    ]
  }'
```

---

## üéØ Mod√®les Recommand√©s

### Pour Ollama (Local)

```bash
# Rapide et l√©ger (2GB)
ollama pull llama3.2:3b

# √âquilibr√© - RECOMMAND√â (4.7GB)
ollama pull llama3.1:8b

# Puissant (40GB - si vous avez la RAM)
ollama pull llama3.1:70b

# Optimis√© fran√ßais (4GB)
ollama pull vigogne:7b-instruct
```

### Pour Groq (API Gratuite)

- `llama3-70b-8192` - **Le meilleur** (recommand√©)
- `llama3-8b-8192` - Rapide et efficace
- `mixtral-8x7b-32768` - Bon pour contenu long
- `gemma-7b-it` - Alternative l√©g√®re

### Pour HuggingFace (API Gratuite)

- `mistralai/Mistral-7B-Instruct-v0.2` - Excellent
- `meta-llama/Llama-2-7b-chat-hf` - Stable
- `tiiuae/falcon-7b-instruct` - Alternative

---

## üí° Bonnes Pratiques

### 1. Cache Redis (Recommand√©)

```bash
# D√©marrer Redis
docker-compose -f docker-compose.redis.yml up -d

# Configurer .env
REDIS_URL=redis://localhost:6379
```

**√âconomies : 90% des requ√™tes en moins !**

### 2. Param√®tres de G√©n√©ration

```typescript
// Pr√©cis et factuel
{ temperature: 0.3, tone: 'professional' }

// √âquilibr√©
{ temperature: 0.7, tone: 'friendly' }

// Cr√©atif
{ temperature: 1.0, tone: 'persuasive' }
```

### 3. Longueurs Optimales

- **short** : 100-200 mots (m√©ta-descriptions, tweets)
- **medium** : 250-500 mots (descriptions produits)
- **long** : 500-1000 mots (articles, guides)

---

## üîß D√©pannage

### Ollama ne d√©marre pas

```bash
# V√©rifier le processus
ps aux | grep ollama

# Red√©marrer
pkill ollama
ollama serve &

# V√©rifier les logs
journalctl -u ollama -f
```

### Erreur "Model not found"

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.1:8b
```

### Groq retourne 429 (Rate Limit)

**Solution :** Vous avez atteint la limite gratuite (14,400/jour).
- Attendez 24h
- Basculez sur Ollama (local, illimit√©)
- Ajoutez HuggingFace en backup

### G√©n√©ration trop lente

**Solutions :**
1. Utilisez Groq (ultra-rapide)
2. R√©duisez `maxLength`
3. Utilisez un mod√®le plus petit (`llama3.2:3b`)
4. Activez le cache Redis

---

## üìä Monitoring et Stats

### V√©rifier le cache

```bash
curl http://localhost:5001/api/ai-content/cache/stats
```

### Lister les templates

```bash
curl http://localhost:5001/api/ai-content/prompts
```

### Health Check

```bash
curl http://localhost:5001/api/health
```

---

## üöÄ D√©marrage du Syst√®me

```bash
# 1. D√©marrer Ollama (si utilis√©)
ollama serve &

# 2. D√©marrer Redis (optionnel)
docker-compose -f docker-compose.redis.yml up -d

# 3. D√©marrer le backend
cd backend && npm run dev

# 4. D√©marrer le frontend
cd frontend && npm run dev
```

---

## üìö Ressources

- üìñ Documentation compl√®te : `AI-CONTENT-GENERATOR-DOCS.md`
- üîß Providers gratuits : `AI-PROVIDERS-GRATUITS.md`
- ü§ñ Ollama : https://ollama.com
- ‚ö° Groq : https://console.groq.com
- ü§ó HuggingFace : https://huggingface.co

---

## ‚úÖ Checklist de Mise en Production

- [ ] Ollama install√© et test√©
- [ ] Cl√© Groq configur√©e (backup)
- [ ] Redis activ√© pour le cache
- [ ] Tests de g√©n√©ration r√©ussis
- [ ] Monitoring configur√©
- [ ] Templates personnalis√©s cr√©√©s
- [ ] Documentation √©quipe mise √† jour

---

## üéâ C'est Pr√™t !

Vous avez maintenant un syst√®me de g√©n√©ration de contenu IA **professionnel**, **gratuit**, et **illimit√©** !

**Commencez simplement :**
```bash
ollama run llama3.1:8b "Bonjour, g√©n√®re-moi du contenu !"
```

üöÄ **Bonne g√©n√©ration !**
