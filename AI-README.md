# ü§ñ Syst√®me de G√©n√©ration de Contenu IA - README

## üìã Vue d'ensemble

Syst√®me complet de g√©n√©ration de contenu intelligent utilisant plusieurs providers IA **100% GRATUITS** :
- **Ollama** (local, illimit√©)
- **Groq** (API gratuite, ultra-rapide)
- **HuggingFace** (API gratuite)
- OpenAI (optionnel, payant)

## üöÄ D√©marrage Rapide (2 minutes)

```bash
# 1. Installer Ollama (local, gratuit)
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
ollama pull llama3.1:8b

# 2. Configurer l'environnement
cp .env.ai.example .env
# √âditer .env et d√©finir AI_PROVIDER=ollama

# 3. Tester
./test-ai-content.sh
```

**‚úÖ C'est tout ! Vous avez maintenant un syst√®me IA gratuit et illimit√©.**

## üì¶ Fichiers Cr√©√©s

### Backend
```
backend/src/modules/ai-content/
‚îú‚îÄ‚îÄ ai-content.module.ts              # Module principal
‚îú‚îÄ‚îÄ ai-content.controller.ts          # API endpoints
‚îú‚îÄ‚îÄ ai-content.service.ts             # Service avec auto-d√©tection
‚îú‚îÄ‚îÄ ai-content-cache.service.ts       # Cache Redis
‚îú‚îÄ‚îÄ prompt-template.controller.ts     # Gestion templates
‚îú‚îÄ‚îÄ prompt-template.service.ts        # Service templates
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ ollama.provider.ts            # Provider Ollama (local)
‚îÇ   ‚îú‚îÄ‚îÄ groq.provider.ts              # Provider Groq (gratuit)
‚îÇ   ‚îî‚îÄ‚îÄ huggingface.provider.ts       # Provider HuggingFace (gratuit)
‚îú‚îÄ‚îÄ dto/
‚îÇ   ‚îú‚îÄ‚îÄ generate-content.dto.ts       # DTOs g√©n√©ration
‚îÇ   ‚îî‚îÄ‚îÄ prompt-template.dto.ts        # DTOs templates
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ content-templates.ts          # Templates pr√©d√©finis
‚îî‚îÄ‚îÄ __tests__/
    ‚îî‚îÄ‚îÄ ai-content.service.spec.ts    # Tests unitaires
```

### Frontend
```
frontend/app/
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useAiContent.ts               # Hook React
‚îî‚îÄ‚îÄ components/ai/
    ‚îú‚îÄ‚îÄ AiContentGenerator.tsx        # Composant g√©n√©rique
    ‚îú‚îÄ‚îÄ ProductDescriptionGenerator.tsx # Sp√©cialis√© produits
    ‚îî‚îÄ‚îÄ SEOMetaGenerator.tsx          # Sp√©cialis√© SEO
```

### Documentation & Scripts
```
.
‚îú‚îÄ‚îÄ AI-QUICK-START.md                 # Guide rapide (ce fichier)
‚îú‚îÄ‚îÄ AI-CONTENT-GENERATOR-DOCS.md      # Documentation compl√®te
‚îú‚îÄ‚îÄ AI-PROVIDERS-GRATUITS.md          # Guide des providers gratuits
‚îú‚îÄ‚îÄ .env.ai.example                   # Configuration exemple
‚îú‚îÄ‚îÄ install-ai-providers.sh           # Script installation auto
‚îî‚îÄ‚îÄ test-ai-content.sh                # Script de tests
```

## üéØ Types de Contenu Support√©s

1. **product_description** - Descriptions de produits optimis√©es
2. **seo_meta** - M√©ta-descriptions SEO (150-160 chars)
3. **marketing_copy** - Textes marketing persuasifs
4. **blog_article** - Articles de blog structur√©s
5. **social_media** - Posts pour r√©seaux sociaux
6. **email_campaign** - Emails marketing

## üîå API Endpoints

```
POST /api/ai-content/generate                      # G√©n√©ration g√©n√©rique
POST /api/ai-content/generate/product-description  # Description produit
POST /api/ai-content/generate/seo-meta            # M√©ta SEO
POST /api/ai-content/generate/batch               # G√©n√©ration par lots

GET  /api/ai-content/prompts                      # Lister templates
POST /api/ai-content/prompts                      # Cr√©er template
GET  /api/ai-content/prompts/:id                  # Obtenir template
POST /api/ai-content/prompts/:id/test             # Tester template
DELETE /api/ai-content/prompts/:id                # Supprimer template
```

## ‚öôÔ∏è Configuration

### Option 1: Auto-D√©tection (Recommand√©)

```bash
# .env
AI_PROVIDER=auto

# Configure tous les providers disponibles
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

GROQ_API_KEY=gsk_...
GROQ_MODEL=llama3-70b-8192

HUGGINGFACE_API_KEY=hf_...
```

Le syst√®me essaiera automatiquement : Ollama ‚Üí Groq ‚Üí HuggingFace ‚Üí OpenAI

### Option 2: Provider Sp√©cifique

```bash
# Utiliser uniquement Ollama (local)
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# OU utiliser Groq (gratuit, ultra-rapide)
AI_PROVIDER=groq
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama3-70b-8192
```

## üìä Comparaison des Providers

| Provider | Co√ªt | Setup | Vitesse | Qualit√© | Hors ligne |
|----------|------|-------|---------|---------|------------|
| **Ollama** | üíØ Gratuit ‚àû | 5 min | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ |
| **Groq** | üíØ Gratuit* | 2 min | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| **HuggingFace** | üíØ Gratuit* | 3 min | ‚ö° | ‚≠ê‚≠ê‚≠ê | ‚ùå |
| OpenAI | üí∞ ~20‚Ç¨/mois | 2 min | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |

*14,400 req/jour pour Groq, quota limit√© pour HuggingFace

## üé® Exemples d'Utilisation

### Backend (cURL)

```bash
# Description de produit
curl -X POST http://localhost:5001/api/ai-content/generate/product-description \
  -H "Content-Type: application/json" \
  -d '{
    "productName": "Vanne papillon motoris√©e DN50",
    "features": ["Corps fonte", "Motorisation 24V"],
    "tone": "professional",
    "length": "medium"
  }'
```

### Frontend (React)

```typescript
import { useAiContent } from '~/hooks/useAiContent';

function MyComponent() {
  const { generateProductDescription, isLoading } = useAiContent();

  const generate = async () => {
    const result = await generateProductDescription({
      productName: 'Vanne papillon DN50',
      features: ['Corps fonte', 'Motorisation 24V'],
      tone: 'professional',
      length: 'medium',
    });
    
    console.log(result.content);
  };

  return (
    <button onClick={generate} disabled={isLoading}>
      ‚ú® G√©n√©rer avec IA
    </button>
  );
}
```

### Composant UI

```typescript
import { AiContentGenerator } from '~/components/ai/AiContentGenerator';

export default function AdminPage() {
  return (
    <AiContentGenerator
      onContentGenerated={(content) => {
        console.log('G√©n√©r√©:', content);
      }}
    />
  );
}
```

## üß™ Tests

```bash
# Lancer tous les tests
./test-ai-content.sh

# Tests unitaires
cd backend && npm test ai-content

# Test manuel
ollama run llama3.1:8b "Test de g√©n√©ration"
```

## üîß D√©pannage

### Ollama ne d√©marre pas
```bash
ps aux | grep ollama
pkill ollama && ollama serve &
```

### Mod√®le non trouv√©
```bash
ollama list
ollama pull llama3.1:8b
```

### Erreur de connexion
```bash
curl http://localhost:11434/api/tags
# Si erreur : v√©rifier que ollama serve est lanc√©
```

### Cache Redis inactif
```bash
docker-compose -f docker-compose.redis.yml up -d
redis-cli ping
```

## üìö Documentation Compl√®te

- **Guide rapide** : `AI-QUICK-START.md` (ce fichier)
- **Documentation compl√®te** : `AI-CONTENT-GENERATOR-DOCS.md`
- **Providers gratuits** : `AI-PROVIDERS-GRATUITS.md`

## üéØ Roadmap

- [x] Support Ollama (local)
- [x] Support Groq (gratuit)
- [x] Support HuggingFace (gratuit)
- [x] Auto-d√©tection providers
- [x] Cache Redis
- [x] Templates personnalisables
- [x] G√©n√©ration par lots
- [x] Composants React
- [ ] Support Anthropic Claude
- [ ] Interface admin pour templates
- [ ] Analytics et monitoring
- [ ] A/B testing de prompts

## ü§ù Support

**Probl√®mes courants :**
1. V√©rifier les logs : `tail -f logs/backend.log`
2. Tester Ollama : `ollama run llama3.1:8b "test"`
3. V√©rifier l'API : `curl http://localhost:5001/api/health`

**Resources :**
- Ollama : https://ollama.com
- Groq : https://console.groq.com
- HuggingFace : https://huggingface.co

## üéâ Conclusion

Vous avez maintenant un syst√®me de g√©n√©ration de contenu IA :
- ‚úÖ **100% GRATUIT** (Ollama + Groq)
- ‚úÖ **Illimit√©** (Ollama local)
- ‚úÖ **Ultra-rapide** (Groq)
- ‚úÖ **Pr√™t pour la production**
- ‚úÖ **√âvolutif** (ajoutez des providers facilement)

**Commencez maintenant :**
```bash
./install-ai-providers.sh
```

üöÄ **Bonne g√©n√©ration !**
