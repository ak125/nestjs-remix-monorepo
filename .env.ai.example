
# ==========================================
# ü§ñ CONFIGURATION IA - G√âN√âRATION DE CONTENU
# ==========================================

# === STRAT√âGIE DE PROVIDER ===
# auto = D√©tection automatique du meilleur provider disponible
# ollama = Utiliser Ollama (local)
# groq = Utiliser Groq (API gratuite)
# huggingface = Utiliser HuggingFace (API gratuite)
# openai = Utiliser OpenAI (payant)
AI_PROVIDER=auto

# === OLLAMA (Local - GRATUIT illimit√©) ===
# Installation: curl -fsSL https://ollama.com/install.sh | sh
# D√©marrer: ollama serve &
# T√©l√©charger mod√®le: ollama pull llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# === GROQ (API gratuite - Ultra rapide) ===
# Obtenir une cl√© gratuite sur: https://console.groq.com
# Quota: 14,400 requ√™tes/jour GRATUIT
# Le plus rapide: 500 tokens/seconde!
GROQ_API_KEY=
GROQ_MODEL=llama3-70b-8192

# === HUGGINGFACE (API gratuite - Backup) ===
# Obtenir une cl√© gratuite sur: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# === OPENAI (Payant - Optionnel) ===
# Uniquement si vous avez d√©j√† un compte OpenAI
OPENAI_API_KEY=

# === CACHE REDIS (Recommand√©) ===
# Active le cache pour r√©duire les co√ªts et acc√©l√©rer
# docker-compose -f docker-compose.redis.yml up -d
REDIS_URL=redis://localhost:6379
