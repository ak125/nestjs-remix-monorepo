# üöÄ Guide de d√©marrage rapide - A/B Testing Crawl Budget

## üìã Pr√©requis

1. **Supabase** configur√© avec les tables cr√©√©es
2. **Google Cloud Project** avec:
   - Search Console API activ√©e
   - Analytics Data API activ√©e
   - Service Account cr√©√© avec les permissions
3. **Variables d'environnement** configur√©es

## üõ†Ô∏è Installation

### 1. Cr√©er les tables Supabase

```sql
-- Ex√©cuter le script SQL
psql -h your-project.supabase.co -U postgres -d postgres -f backend/supabase/migrations/20251027_crawl_budget_experiments.sql
```

Ou via le dashboard Supabase:
- Aller dans SQL Editor
- Copier/coller le contenu de `20251027_crawl_budget_experiments.sql`
- Ex√©cuter

### 2. Installer les d√©pendances

```bash
cd backend
npm install @supabase/supabase-js
npm install googleapis @google-analytics/data
```

### 3. Configurer les variables d'environnement

```bash
cp .env.crawl-budget.example .env
# √âditer .env avec vos credentials
```

### 4. Configurer Google Cloud Service Account

1. Cr√©er un Service Account dans Google Cloud Console
2. Activer les APIs:
   - Google Search Console API
   - Google Analytics Data API
3. T√©l√©charger le fichier JSON des credentials
4. Extraire `client_email` et `private_key`
5. Ajouter le Service Account comme utilisateur dans:
   - Google Search Console (automecanik.com)
   - Google Analytics 4 (propri√©t√©)

## üß™ Cr√©er votre premi√®re exp√©rience

### 1. Cr√©er une exp√©rience

```bash
curl -X POST http://localhost:3000/seo-logs/crawl-budget/experiments \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test exclusion pneus anciens",
    "description": "Exclure 10k URLs de pneus d'occasion pour am√©liorer l'indexation",
    "action": "exclude",
    "targetFamilies": ["PNEU_VIEUX", "PNEU_OCCASION"],
    "durationDays": 30
  }'
```

R√©ponse:
```json
{
  "success": true,
  "message": "Exp√©rience cr√©√©e avec succ√®s",
  "data": {
    "id": "abc123-def456",
    "name": "Test exclusion pneus anciens",
    "status": "draft",
    "baseline": {
      "period": "30d",
      "crawl": {
        "totalCrawledUrls": 1200,
        "indexationRate": 85
      }
    }
  }
}
```

### 2. T√©l√©charger le sitemap filtr√©

```bash
curl http://localhost:3000/seo-logs/crawl-budget/experiments/abc123-def456/sitemap.xml \
  > sitemap-experiment.xml
```

### 3. Soumettre √† Google Search Console

**M√©thode 1: Via le dashboard GSC**
- Ouvrir https://search.google.com/search-console
- S√©lectionner `automecanik.com`
- Aller dans **Sitemaps**
- Ajouter le sitemap: `https://automecanik.com/sitemap-experiment.xml`

**M√©thode 2: Via API** (TODO: impl√©menter l'endpoint automatique)

### 4. Activer l'exp√©rience

```bash
curl -X PATCH http://localhost:3000/seo-logs/crawl-budget/experiments/abc123-def456/status \
  -H "Content-Type: application/json" \
  -d '{"status": "running"}'
```

### 5. Collecter les m√©triques quotidiennes

**Automatique** (via cron ou BullMQ):
```bash
# Ajouter un job quotidien pour collecter m√©triques
```

**Manuel**:
```bash
curl -X POST http://localhost:3000/seo-logs/crawl-budget/experiments/abc123-def456/collect-metrics
```

### 6. Voir les m√©triques

```bash
curl http://localhost:3000/seo-logs/crawl-budget/experiments/abc123-def456/metrics?period=7d | jq
```

### 7. Obtenir les recommandations

```bash
curl http://localhost:3000/seo-logs/crawl-budget/experiments/abc123-def456/recommendations | jq
```

R√©ponse:
```json
{
  "success": true,
  "data": [
    {
      "action": "KEEP_EXCLUSION",
      "reason": "L'indexation s'est am√©lior√©e de +8.5%",
      "confidence": 0.9
    }
  ]
}
```

## üìä Endpoints disponibles

| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| POST | `/experiments` | Cr√©er exp√©rience |
| GET | `/experiments` | Liste exp√©riences |
| GET | `/experiments/:id` | D√©tails exp√©rience |
| GET | `/experiments/:id/metrics` | M√©triques |
| PATCH | `/experiments/:id/status` | Changer statut |
| GET | `/experiments/:id/sitemap.xml` | Sitemap filtr√© |
| GET | `/experiments/:id/recommendations` | Recommandations |
| POST | `/experiments/:id/collect-metrics` | Collecter m√©triques |
| GET | `/stats` | Stats globales |

## üîç V√©rifications

### V√©rifier les tables Supabase

```sql
SELECT * FROM crawl_budget_experiments ORDER BY created_at DESC LIMIT 5;
SELECT * FROM crawl_budget_metrics ORDER BY date DESC LIMIT 10;
```

### V√©rifier les logs backend

```bash
# Chercher les logs de collecte baseline
grep "Exp√©rience cr√©√©e" logs/backend.log

# Chercher les logs GSC/GA4
grep "GSC API\|GA4 API" logs/backend.log
```

### Tester GSC API

```bash
# TODO: Endpoint de test
curl http://localhost:3000/seo-logs/crawl-budget/test/gsc
```

### Tester GA4 API

```bash
# TODO: Endpoint de test
curl http://localhost:3000/seo-logs/crawl-budget/test/ga4
```

## üêõ Troubleshooting

### Erreur: "SUPABASE_URL not set"
```bash
# V√©rifier .env
cat backend/.env | grep SUPABASE
```

### Erreur: "GSC API: unauthorized"
```bash
# V√©rifier que le Service Account est ajout√© dans GSC
# V√©rifier que la cl√© priv√©e est correcte (avec \n)
```

### Erreur: "No metrics collected"
```bash
# V√©rifier que l'exp√©rience est en statut "running"
curl http://localhost:3000/seo-logs/crawl-budget/experiments/YOUR_ID | jq '.data.status'

# Collecter manuellement
curl -X POST http://localhost:3000/seo-logs/crawl-budget/experiments/YOUR_ID/collect-metrics
```

## üìö Prochaines √©tapes

1. Impl√©menter vraie int√©gration GSC API (remplacer mock data)
2. Impl√©menter vraie int√©gration GA4 API (remplacer mock data)
3. Ajouter collecte automatique quotidienne (BullMQ)
4. Cr√©er dashboard Grafana pour visualiser r√©sultats
5. Ajouter alertes (email/Slack) sur changements significatifs
6. Impl√©menter soumission automatique sitemap via GSC API
