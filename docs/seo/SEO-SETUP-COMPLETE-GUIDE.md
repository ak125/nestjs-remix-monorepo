# üöÄ Guide de configuration complet - A/B Testing Crawl Budget

## ‚úÖ Checklist de progression

- [ ] √âtape 1: Cr√©er tables Supabase
- [ ] √âtape 2: Installer d√©pendances npm
- [ ] √âtape 3: Cr√©er Service Account Google Cloud
- [ ] √âtape 4: Activer APIs Google
- [ ] √âtape 5: Configurer credentials .env
- [ ] √âtape 6: Tester premi√®re exp√©rience

---

## üìã √âtape 1: Cr√©er les tables Supabase

### Option A: Via le Dashboard Supabase (Recommand√©)

1. **Ouvrir Supabase** : https://supabase.com/dashboard/project/YOUR_PROJECT

2. **Aller dans SQL Editor** :
   - Menu lat√©ral ‚Üí SQL Editor
   - Cliquer sur "+ New query"

3. **Copier-coller le SQL** :
   ```sql
   -- Copier le contenu de:
   backend/supabase/migrations/20251027_crawl_budget_experiments.sql
   ```

4. **Ex√©cuter** : Cliquer sur "Run" ou `Ctrl+Enter`

5. **V√©rifier** :
   ```sql
   SELECT table_name 
   FROM information_schema.tables 
   WHERE table_schema = 'public' 
   AND table_name IN ('crawl_budget_experiments', 'crawl_budget_metrics');
   ```
   Devrait retourner 2 lignes.

### Option B: Via psql (Ligne de commande)

```bash
# R√©cup√©rer connection string depuis Supabase Dashboard
# Settings ‚Üí Database ‚Üí Connection string ‚Üí URI

psql "postgresql://postgres:[YOUR-PASSWORD]@db.[YOUR-PROJECT-REF].supabase.co:5432/postgres" \
  -f /workspaces/nestjs-remix-monorepo/backend/supabase/migrations/20251027_crawl_budget_experiments.sql
```

### Option C: Via Supabase CLI

```bash
# Installer Supabase CLI
npm install -g supabase

# Lier le projet
supabase link --project-ref YOUR_PROJECT_REF

# Appliquer migration
supabase db push
```

### ‚úÖ V√©rification

```sql
-- Dans SQL Editor Supabase
SELECT COUNT(*) as experiment_count FROM crawl_budget_experiments;
SELECT COUNT(*) as metrics_count FROM crawl_budget_metrics;
```

---

## üì¶ √âtape 2: Installer les d√©pendances npm

```bash
cd /workspaces/nestjs-remix-monorepo/backend

# Installer les packages Google
npm install googleapis @google-analytics/data

# Installer Supabase client
npm install @supabase/supabase-js

# V√©rifier l'installation
npm list googleapis @google-analytics/data @supabase/supabase-js
```

---

## üîë √âtape 3: Cr√©er Service Account Google Cloud

### 3.1 Cr√©er un projet Google Cloud (si n√©cessaire)

1. Aller sur https://console.cloud.google.com/
2. Cliquer sur le s√©lecteur de projet (en haut)
3. "Nouveau projet" ‚Üí Nom: `Automecanik SEO`
4. Cr√©er

### 3.2 Cr√©er le Service Account

1. **Navigation** : IAM & Admin ‚Üí Service Accounts
2. **Cr√©er** : "Create Service Account"
   - Nom: `seo-crawl-budget`
   - Description: `Service account for SEO A/B testing and crawl budget optimization`
3. **R√¥le** : `Viewer` (ou aucun r√¥le, on donnera les permissions sp√©cifiques apr√®s)
4. **Continuer** ‚Üí **Done**

### 3.3 Cr√©er la cl√© JSON

1. Cliquer sur le service account cr√©√©
2. Onglet "Keys" ‚Üí "Add Key" ‚Üí "Create new key"
3. Type: **JSON**
4. T√©l√©charger le fichier `seo-crawl-budget-xxxxx.json`

### 3.4 Extraire les informations

Ouvrir le fichier JSON et copier:
```json
{
  "client_email": "seo-crawl-budget@automecanik-seo-xxxxx.iam.gserviceaccount.com",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQI...\n-----END PRIVATE KEY-----\n"
}
```

‚ö†Ô∏è **Garder ce fichier en s√©curit√©** (ne jamais le commit dans Git)

---

## üîì √âtape 4: Activer les APIs Google

### 4.1 API Library

1. Aller sur https://console.cloud.google.com/apis/library
2. Chercher et activer:

**A. Google Search Console API**
   - Chercher: "Search Console API"
   - Cliquer sur "Google Search Console API"
   - **Enable**

**B. Google Analytics Data API**
   - Chercher: "Analytics Data API"
   - Cliquer sur "Google Analytics Data API"  
   - **Enable**

### 4.2 V√©rifier les APIs activ√©es

```bash
# Via gcloud CLI (optionnel)
gcloud services list --enabled | grep -E "searchconsole|analyticsdata"
```

Devrait afficher:
```
searchconsole.googleapis.com
analyticsdata.googleapis.com
```

---

## üîê √âtape 5: Donner acc√®s au Service Account

### 5.1 Google Search Console

1. Ouvrir https://search.google.com/search-console
2. S√©lectionner `https://www.automecanik.com/`
3. **Param√®tres** (‚öôÔ∏è) ‚Üí **Utilisateurs et autorisations**
4. **Ajouter un utilisateur**:
   - Email: `seo-crawl-budget@automecanik-seo-xxxxx.iam.gserviceaccount.com`
   - Autorisation: **Utilisateur complet** (ou Propri√©taire)
5. **Ajouter**

### 5.2 Google Analytics 4

1. Ouvrir https://analytics.google.com/
2. **Admin** (‚öôÔ∏è en bas √† gauche)
3. Colonne **Propri√©t√©** ‚Üí **Acc√®s √† la propri√©t√©**
4. **Ajouter des utilisateurs** (+)
   - Email: `seo-crawl-budget@automecanik-seo-xxxxx.iam.gserviceaccount.com`
   - R√¥les: **Lecteur** ‚úÖ
   - D√©cocher "Notifier ce nouvel utilisateur par e-mail"
5. **Ajouter**

### 5.3 R√©cup√©rer l'ID de propri√©t√© GA4

1. Google Analytics ‚Üí **Admin**
2. Colonne **Propri√©t√©** ‚Üí **Informations de la propri√©t√©**
3. Copier **ID de la propri√©t√©** (ex: `123456789`)

---

## ‚öôÔ∏è √âtape 6: Configurer les credentials dans .env

```bash
cd /workspaces/nestjs-remix-monorepo/backend

# √âditer .env
nano .env
```

### Ajouter ces lignes :

```bash
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# üß™ SEO CRAWL BUDGET A/B TESTING
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.ey...

# Google Search Console
GSC_SITE_URL=https://www.automecanik.com
GSC_CLIENT_EMAIL=seo-crawl-budget@automecanik-seo-xxxxx.iam.gserviceaccount.com
GSC_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEF...\n-----END PRIVATE KEY-----\n"

# Google Analytics 4
GA4_PROPERTY_ID=123456789
GA4_CLIENT_EMAIL=seo-crawl-budget@automecanik-seo-xxxxx.iam.gserviceaccount.com
GA4_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEF...\n-----END PRIVATE KEY-----\n"

# Sitemap
SITEMAP_BASE_URL=https://automecanik.com
```

### ‚ö†Ô∏è Important pour PRIVATE_KEY

La cl√© priv√©e doit contenir les caract√®res `\n` litt√©raux (pas de vraies nouvelles lignes).

**Si votre cl√© a des vraies nouvelles lignes**, remplacez-les :
```bash
# Exemple de transformation
"-----BEGIN PRIVATE KEY-----
MIIEvQIBADANBgk...
XYZ123
-----END PRIVATE KEY-----"

# Devient :
"-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgk...\nXYZ123\n-----END PRIVATE KEY-----\n"
```

### üîç Trouver SUPABASE_SERVICE_ROLE_KEY

1. Supabase Dashboard ‚Üí Settings ‚Üí API
2. Section "Project API keys"
3. Copier **service_role key** (secret)

---

## üß™ √âtape 7: Tester le syst√®me

### 7.1 Red√©marrer le backend

```bash
cd /workspaces/nestjs-remix-monorepo/backend
npm run dev
```

V√©rifier qu'il n'y a pas d'erreurs de connexion Supabase.

### 7.2 Tester l'acc√®s Supabase

```bash
curl http://localhost:3000/seo-logs/crawl-budget/stats | jq
```

**R√©sultat attendu** :
```json
{
  "success": true,
  "data": {
    "total": 0,
    "running": 0,
    "completed": 0,
    "draft": 0
  }
}
```

### 7.3 Cr√©er votre premi√®re exp√©rience

```bash
curl -X POST http://localhost:3000/seo-logs/crawl-budget/experiments \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test exclusion pneus anciens",
    "description": "Exclure 10000 URLs de pneus d'\''occasion pour am√©liorer le crawl budget",
    "action": "exclude",
    "targetFamilies": ["PNEU_VIEUX", "PNEU_OCCASION"],
    "durationDays": 30
  }' | jq
```

**R√©sultat attendu** :
```json
{
  "success": true,
  "message": "Exp√©rience cr√©√©e avec succ√®s",
  "data": {
    "id": "abc-123-def-456",
    "name": "Test exclusion pneus anciens",
    "status": "draft",
    "baseline": {
      "period": "30d",
      "crawl": {
        "totalCrawledUrls": 1200,
        "indexationRate": 85
      }
    }
  }
}
```

### 7.4 V√©rifier dans Supabase

```sql
-- Dans SQL Editor
SELECT * FROM crawl_budget_experiments ORDER BY created_at DESC LIMIT 1;
```

### 7.5 R√©cup√©rer l'exp√©rience

```bash
# Remplacer {id} par l'ID retourn√©
curl http://localhost:3000/seo-logs/crawl-budget/experiments/{id} | jq
```

### 7.6 T√©l√©charger le sitemap filtr√©

```bash
curl http://localhost:3000/seo-logs/crawl-budget/experiments/{id}/sitemap.xml \
  > /tmp/sitemap-experiment.xml

# V√©rifier
head -20 /tmp/sitemap-experiment.xml
```

### 7.7 Activer l'exp√©rience

```bash
curl -X PATCH http://localhost:3000/seo-logs/crawl-budget/experiments/{id}/status \
  -H "Content-Type: application/json" \
  -d '{"status": "running"}' | jq
```

### 7.8 Collecter les m√©triques (mock data pour l'instant)

```bash
curl -X POST http://localhost:3000/seo-logs/crawl-budget/experiments/{id}/collect-metrics | jq
```

### 7.9 Voir les recommandations

```bash
curl http://localhost:3000/seo-logs/crawl-budget/experiments/{id}/recommendations | jq
```

---

## üêõ Troubleshooting

### Erreur: "relation does not exist"
```bash
# Les tables ne sont pas cr√©√©es
# ‚Üí Retourner √† l'√âtape 1
```

### Erreur: "SUPABASE_URL not set"
```bash
# Variables .env pas charg√©es
grep SUPABASE /workspaces/nestjs-remix-monorepo/backend/.env
```

### Erreur: "GSC API: unauthorized"
```bash
# Service Account pas ajout√© dans GSC
# ‚Üí Retourner √† l'√âtape 5.1
```

### Erreur: "Invalid private key"
```bash
# V√©rifier les \n dans la cl√©
echo $GSC_PRIVATE_KEY | head -c 100
# Devrait afficher: -----BEGIN PRIVATE KEY-----\nMIIE...
```

### Mock data au lieu de vraies donn√©es
```bash
# Normal ! Les services utilisent mock data par d√©faut
# Pour activer les vraies APIs, modifier:
# backend/src/modules/seo-logs/services/crawl-budget-integrations.service.ts
```

---

## ‚úÖ Checklist finale

- [ ] Tables Supabase cr√©√©es et visibles
- [ ] D√©pendances npm install√©es
- [ ] Service Account cr√©√© et cl√© JSON t√©l√©charg√©e
- [ ] APIs activ√©es (Search Console + Analytics Data)
- [ ] Service Account ajout√© dans GSC (Utilisateur complet)
- [ ] Service Account ajout√© dans GA4 (Lecteur)
- [ ] ID de propri√©t√© GA4 r√©cup√©r√©
- [ ] Variables .env configur√©es
- [ ] Backend red√©marr√© sans erreurs
- [ ] Endpoint /stats retourne 200
- [ ] Premi√®re exp√©rience cr√©√©e avec succ√®s
- [ ] Exp√©rience visible dans Supabase
- [ ] Sitemap filtr√© t√©l√©chargeable

---

## üéâ F√©licitations !

Votre syst√®me d'A/B Testing du Crawl Budget est maintenant **op√©rationnel** !

### Prochaines √©tapes

1. **Soumettre le sitemap** √† Google Search Console
2. **Attendre 7 jours** pour collecter des donn√©es
3. **Analyser les recommandations** automatiques
4. **D√©cider** : garder l'exclusion ou r√©int√©grer

### Workflow complet

```
Cr√©er exp√©rience ‚Üí T√©l√©charger sitemap ‚Üí Soumettre GSC ‚Üí Activer
    ‚Üì
Collecter m√©triques quotidiennes (automatique avec BullMQ)
    ‚Üì
Analyser apr√®s 30j ‚Üí Recommandations ‚Üí D√©cision finale
```

üöÄ **Le syst√®me est pr√™t √† optimiser votre crawl budget !**
