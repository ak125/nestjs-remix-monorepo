#!/bin/bash

echo "ðŸ¤– Installation des Providers IA GRATUITS pour votre systÃ¨me de gÃ©nÃ©ration de contenu"
echo "================================================================================="
echo ""

# Couleurs pour l'affichage
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# 1. Installer Ollama (local - gratuit illimitÃ©)
echo -e "${BLUE}ðŸ“¦ Ã‰tape 1/3: Installation d'Ollama (modÃ¨les IA locaux)${NC}"
echo "Cela peut prendre quelques minutes..."

if command -v ollama &> /dev/null; then
    echo -e "${GREEN}âœ… Ollama est dÃ©jÃ  installÃ©${NC}"
else
    curl -fsSL https://ollama.com/install.sh | sh
    echo -e "${GREEN}âœ… Ollama installÃ©${NC}"
fi

# DÃ©marrer Ollama en arriÃ¨re-plan
ollama serve > /dev/null 2>&1 &
sleep 3

# TÃ©lÃ©charger un modÃ¨le optimisÃ©
echo ""
echo -e "${BLUE}ðŸ“¥ TÃ©lÃ©chargement du modÃ¨le Llama 3.1 8B (4.7GB)${NC}"
echo "Ce modÃ¨le offre le meilleur compromis qualitÃ©/vitesse"
ollama pull llama3.1:8b

echo -e "${GREEN}âœ… ModÃ¨le tÃ©lÃ©chargÃ© et prÃªt Ã  l'emploi${NC}"

# 2. Configurer le fichier .env
echo ""
echo -e "${BLUE}âš™ï¸  Ã‰tape 2/3: Configuration du fichier .env${NC}"

# VÃ©rifier si .env existe
if [ ! -f ".env" ]; then
    touch .env
fi

# VÃ©rifier si la configuration IA existe dÃ©jÃ 
if grep -q "AI_PROVIDER" .env; then
    echo -e "${YELLOW}âš ï¸  Configuration IA dÃ©jÃ  prÃ©sente dans .env${NC}"
else
    cat >> .env << 'EOF'

# ==========================================
# ðŸ¤– CONFIGURATION IA - GÃ‰NÃ‰RATION DE CONTENU
# ==========================================

# Provider IA (auto = dÃ©tection automatique du meilleur disponible)
AI_PROVIDER=auto

# === OLLAMA (Local - GRATUIT illimitÃ©) ===
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# === GROQ (API gratuite - Ultra rapide) ===
# Obtenir une clÃ© gratuite sur: https://console.groq.com
# 14,400 requÃªtes/jour gratuites - Le plus rapide!
GROQ_API_KEY=
GROQ_MODEL=llama3-70b-8192

# === HUGGINGFACE (API gratuite - Backup) ===
# Obtenir une clÃ© gratuite sur: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# === OPENAI (Payant - Optionnel) ===
# Uniquement si vous avez dÃ©jÃ  un compte OpenAI
OPENAI_API_KEY=
EOF
    echo -e "${GREEN}âœ… Configuration ajoutÃ©e dans .env${NC}"
fi

# 3. Instructions finales
echo ""
echo -e "${BLUE}ðŸŽ¯ Ã‰tape 3/3: Prochaines actions${NC}"
echo "================================================================================="
echo ""
echo -e "${GREEN}âœ… Ollama est installÃ© et prÃªt!${NC} Vous pouvez dÃ©jÃ  gÃ©nÃ©rer du contenu en local."
echo ""
echo -e "${YELLOW}ðŸ“ Pour amÃ©liorer les performances (RECOMMANDÃ‰):${NC}"
echo ""
echo "1. CrÃ©er un compte Groq GRATUIT (2 minutes):"
echo "   ðŸ‘‰ https://console.groq.com"
echo ""
echo "2. Copier votre clÃ© API gratuite"
echo ""
echo "3. L'ajouter dans le fichier .env:"
echo "   GROQ_API_KEY=gsk_votre_clÃ©_ici"
echo ""
echo -e "${BLUE}ðŸ’¡ Avec Groq, vous aurez:${NC}"
echo "   â€¢ 14,400 requÃªtes/jour gratuites"
echo "   â€¢ GÃ©nÃ©ration ultra-rapide (500 tokens/seconde!)"
echo "   â€¢ Meilleure qualitÃ© de contenu"
echo ""
echo "================================================================================="
echo ""
echo -e "${GREEN}ðŸš€ Installation terminÃ©e!${NC}"
echo ""
echo -e "${YELLOW}Test rapide:${NC}"
echo "  ollama run llama3.1:8b \"Ã‰cris une description pour une vanne papillon\""
echo ""
echo -e "${YELLOW}DÃ©marrer le backend:${NC}"
echo "  cd backend && npm run dev"
echo ""
echo "================================================================================="
